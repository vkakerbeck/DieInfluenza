{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Vivi\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "%pylab inline\n",
    "import tensorflow as tf\n",
    "#from src.setup.config import db\n",
    "from itertools import groupby\n",
    "import pandas as pd\n",
    "from collections import OrderedDict\n",
    "from datetime import date\n",
    "import pyodbc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing (can be skipped after done once)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "records=pd.read_csv('C:/Users/Vivi/Desktop/hack4health_RKI-data-scripts/survstat/csv/cam_5y.csv',encoding='latin-1')\n",
    "CoNew = pd.read_csv('C:/Users/Vivi/Desktop/hack4health_RKI-data-scripts/DieInfluenza/CoordBins.csv',encoding='latin-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "recordsX=records[['county','week','count','incidence']]\n",
    "gb = recordsX.groupby(['week','county']).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gb.index[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Vivi\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:9: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if __name__ == '__main__':\n"
     ]
    }
   ],
   "source": [
    "newDF = pd.DataFrame(columns=['week','year','county','count','incidence','X','Y','BinX','BinY','LocCounty'])\n",
    "x = 0\n",
    "for i in gb.index:\n",
    "    week = i[0]\n",
    "    counties = i[1]\n",
    "    counts = gb.values[x][0]\n",
    "    incidences = gb.values[x][1]\n",
    "    loc = CoNew[CoNew['county']==counties]\n",
    "    if loc['X'].values>0:\n",
    "        newDF=newDF.append({'week': str(week)[7:],'year':str(week)[:4],'county':str(counties),'count':str(counts),'incidence':str(incidences),'X':loc['X'].values,'Y':loc['Y'].values,'BinX':loc['BinX'].values,'BinY':loc['BinY'].values,'LocCounty':loc['county'].values}, ignore_index=True)    \n",
    "    try:\n",
    "        with open('StomachVirusInput.csv', 'a') as f:\n",
    "            newDF.to_csv(f, header=False)\n",
    "    except:\n",
    "        pass\n",
    "    newDF=pd.DataFrame(columns=['week','year','county','count','incidence','X','Y','BinX','BinY','LocCounty'])\n",
    "    x = x+1\n",
    "#newDF.to_csv('StomachVirusInput.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "newDF = pd.read_csv('StomachVirusInput.csv',encoding='latin-1')\n",
    "newDF.columns = ['ID','week','year','county','count','incidence','X','Y','BinX','BinY','LocCounty']#.set_index('Unnamed: 0')\n",
    "newDF = newDF.dropna()\n",
    "newDF = newDF[['week','year','county','count','incidence','X','Y','BinX','BinY']].reset_index(drop=True)\n",
    "newDF.to_csv('StomachVirusNew.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "newDF = pd.read_csv('StomachVirusInput.csv',encoding='latin-1')\n",
    "newDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test = newDF[newDF['year']==2001][newDF[newDF['year']==2001]['week']==2]['incidence']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "6000\n",
      "7000\n",
      "8000\n",
      "9000\n",
      "10000\n",
      "11000\n",
      "12000\n",
      "13000\n",
      "14000\n",
      "15000\n",
      "16000\n",
      "17000\n",
      "18000\n",
      "19000\n",
      "20000\n",
      "21000\n",
      "22000\n",
      "23000\n",
      "24000\n",
      "25000\n",
      "26000\n",
      "27000\n",
      "28000\n",
      "29000\n",
      "30000\n",
      "31000\n",
      "32000\n",
      "33000\n",
      "34000\n",
      "35000\n",
      "36000\n",
      "37000\n",
      "38000\n",
      "39000\n",
      "40000\n",
      "41000\n",
      "42000\n",
      "43000\n",
      "44000\n",
      "45000\n",
      "46000\n",
      "47000\n",
      "48000\n",
      "49000\n",
      "50000\n",
      "51000\n",
      "52000\n",
      "53000\n",
      "54000\n",
      "55000\n",
      "56000\n",
      "57000\n",
      "58000\n",
      "59000\n",
      "60000\n",
      "61000\n",
      "62000\n",
      "63000\n",
      "64000\n",
      "65000\n",
      "66000\n",
      "67000\n",
      "68000\n",
      "69000\n",
      "70000\n",
      "71000\n",
      "72000\n",
      "73000\n",
      "74000\n",
      "75000\n",
      "76000\n",
      "77000\n",
      "78000\n",
      "79000\n",
      "80000\n",
      "81000\n",
      "82000\n",
      "83000\n",
      "84000\n",
      "85000\n",
      "86000\n",
      "87000\n",
      "88000\n",
      "89000\n",
      "90000\n",
      "91000\n",
      "92000\n",
      "93000\n",
      "94000\n",
      "95000\n",
      "96000\n",
      "97000\n",
      "98000\n",
      "99000\n",
      "100000\n",
      "101000\n",
      "102000\n",
      "103000\n",
      "104000\n",
      "105000\n",
      "106000\n",
      "107000\n",
      "108000\n",
      "109000\n",
      "110000\n",
      "111000\n",
      "112000\n",
      "113000\n",
      "114000\n",
      "115000\n",
      "116000\n",
      "117000\n",
      "118000\n",
      "119000\n",
      "120000\n",
      "121000\n",
      "122000\n",
      "123000\n",
      "124000\n",
      "125000\n",
      "126000\n",
      "127000\n",
      "128000\n",
      "129000\n",
      "130000\n",
      "131000\n",
      "132000\n",
      "133000\n",
      "134000\n",
      "135000\n",
      "136000\n",
      "137000\n",
      "138000\n",
      "139000\n",
      "140000\n",
      "141000\n",
      "142000\n",
      "143000\n",
      "144000\n",
      "145000\n",
      "146000\n",
      "147000\n",
      "148000\n",
      "149000\n",
      "150000\n",
      "151000\n",
      "152000\n",
      "153000\n",
      "154000\n",
      "155000\n",
      "156000\n",
      "157000\n",
      "158000\n",
      "159000\n",
      "160000\n",
      "161000\n",
      "162000\n",
      "163000\n",
      "164000\n",
      "165000\n",
      "166000\n",
      "167000\n",
      "168000\n",
      "169000\n",
      "170000\n",
      "171000\n",
      "172000\n",
      "173000\n",
      "174000\n",
      "175000\n",
      "176000\n",
      "177000\n",
      "178000\n",
      "179000\n",
      "180000\n",
      "181000\n",
      "182000\n",
      "183000\n",
      "184000\n",
      "185000\n",
      "186000\n",
      "187000\n",
      "188000\n",
      "189000\n",
      "190000\n",
      "191000\n",
      "192000\n",
      "193000\n",
      "194000\n",
      "195000\n",
      "196000\n",
      "197000\n",
      "198000\n",
      "199000\n",
      "200000\n",
      "201000\n",
      "202000\n",
      "203000\n",
      "204000\n",
      "205000\n",
      "206000\n",
      "207000\n",
      "208000\n",
      "209000\n",
      "210000\n",
      "211000\n",
      "212000\n",
      "213000\n",
      "214000\n",
      "215000\n",
      "216000\n",
      "217000\n",
      "218000\n",
      "219000\n",
      "220000\n",
      "221000\n",
      "222000\n",
      "223000\n",
      "224000\n",
      "225000\n",
      "226000\n",
      "227000\n",
      "228000\n",
      "229000\n",
      "230000\n",
      "231000\n",
      "232000\n",
      "233000\n",
      "234000\n",
      "235000\n",
      "236000\n",
      "237000\n",
      "238000\n",
      "239000\n",
      "240000\n",
      "241000\n",
      "242000\n",
      "243000\n",
      "244000\n",
      "245000\n",
      "246000\n",
      "247000\n",
      "248000\n",
      "249000\n",
      "250000\n",
      "251000\n",
      "252000\n",
      "253000\n",
      "254000\n",
      "255000\n",
      "256000\n",
      "257000\n",
      "258000\n",
      "259000\n",
      "260000\n",
      "261000\n",
      "262000\n",
      "263000\n"
     ]
    }
   ],
   "source": [
    "def FindLabels(data):\n",
    "    LabelDF = pd.DataFrame(columns=['count','incidence','county'])\n",
    "    for w in data.index:\n",
    "        try:\n",
    "            thisyear = data.iloc[w]['year']\n",
    "            thisweek = data.iloc[w]['week']\n",
    "            thisarea = data.iloc[w]['county']\n",
    "            if int(thisweek)<52:\n",
    "                nextweek = int(thisweek)+1\n",
    "                nextyear = int(thisyear)\n",
    "            else:\n",
    "                nextweek = 1\n",
    "                nextyear = int(thisyear)+1\n",
    "            nextdf = data[data['year']==nextyear][data[data['year']==nextyear]['week']==nextweek]\n",
    "            nextcount = np.sum(nextdf[nextdf['county']==thisarea]['count'])\n",
    "            if nextcount>0:\n",
    "                nextincidence = np.sum(nextdf[nextdf['county']==thisarea]['incidence'])\n",
    "                nextcounty = nextdf[nextdf['county']==thisarea]['county'].values[0]\n",
    "            else:\n",
    "                nextincidence=0  \n",
    "                nextcounty = 'no Data'\n",
    "        except:\n",
    "            nextcount=0\n",
    "            nextincidence=0  \n",
    "            nextcounty = 'no Data'\n",
    "        LabelDF=LabelDF.append({'count':nextcount,'incidence':nextincidence,'county':nextcounty}, ignore_index=True)\n",
    "        if (w % 1000 == 0):\n",
    "            print(w)\n",
    "    return LabelDF\n",
    "        \n",
    "label = FindLabels(newDF)\n",
    "label.to_csv('StomachOutput.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Processed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Labels=pd.read_csv('StomachOutput.csv',encoding='latin-1').set_index('Unnamed: 0')\n",
    "Data=pd.read_csv('StomachVirusInput.csv',encoding='latin-1').set_index('Unnamed: 0')\n",
    "Data = Data[['week','count','incidence','X','Y']]\n",
    "dfboth = Data.join(Labels,lsuffix='x',how='outer').dropna()\n",
    "Data = dfboth[['week','countx','incidencex','X','Y']].rename(index=str, columns={\"countx\": \"count\", \"incidencex\": \"incidence\"})\n",
    "Labels = dfboth[['count','incidence']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalize Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn.preprocessing import normalize\n",
    "normalized = newDF\n",
    "\"\"\"x = normalized['BinX']\n",
    "normBinX =sklearn.preprocessing.normalize([x],axis=1)\n",
    "normalized['BinX']=normBinX[0]\n",
    "x = normalized['BinY']\n",
    "normBinY =sklearn.preprocessing.normalize([x])\n",
    "normalized['BinY']=normBinY[0]\n",
    "x = normalized['X']\n",
    "normX =sklearn.preprocessing.normalize([x])\n",
    "normalized['X']=normX[0]\n",
    "x = normalized['Y']\n",
    "normY =sklearn.preprocessing.normalize([x])\n",
    "normalized['Y']=normY[0]\"\"\"\n",
    "x = normalized['count']\n",
    "normcount =sklearn.preprocessing.normalize([x])\n",
    "normalized['count']=normcount[0]\n",
    "x = normalized['incidence']\n",
    "normInc =sklearn.preprocessing.normalize([x])\n",
    "normalized['incidence']=normInc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "normLabels = Labels\n",
    "x = normLabels['count']\n",
    "normcount =sklearn.preprocessing.normalize([x])\n",
    "normLabels['count']=normcount[0]\n",
    "x = normLabels['incidence']\n",
    "normInc =sklearn.preprocessing.normalize([x])\n",
    "normLabels['incidence']=normcount[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lenall = Data.shape[0]\n",
    "trainSize = lenall//3*2#define sizes for data sets (train: 2/3, val:1/6, test:1/6 of whole dataset) \n",
    "valSize = lenall//6\n",
    "\n",
    "#Split data into train, val and test set------------------------------------------\n",
    "trainingData = Data.iloc[0:trainSize].reset_index(drop=True)\n",
    "trainingLabels = Labels.iloc[0:trainSize].reset_index(drop=True)\n",
    "validationData = Data.iloc[trainSize+1:trainSize+valSize].reset_index(drop=True)\n",
    "validationLabels = Labels.iloc[trainSize+1:trainSize+valSize].reset_index(drop=True)\n",
    "testData = Data.iloc[trainSize+valSize+1:].reset_index(drop=True)\n",
    "testLabels = Labels.iloc[trainSize+valSize+1:].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Method to get batches------------------------------------------------------------\n",
    "def getBatches(batchsize,typeBatch):\n",
    "    if typeBatch==\"train\":\n",
    "        numBatches = shape(trainingData)[0]//batchsize\n",
    "        Batches=[]\n",
    "        for count in range(numBatches-1):\n",
    "            batch = trainingData[count*batchsize:(count+1)*batchsize]\n",
    "            labels = trainingLabels[count*batchsize:(count+1)*batchsize]\n",
    "            b,l=formatBatch(batch,labels,batchsize)\n",
    "            Batches.append([b,l])\n",
    "    elif typeBatch==\"val\":\n",
    "        batch = validationData.sample(batchsize)\n",
    "        labels = validationLabels.iloc[batch.index]\n",
    "        b,l=formatBatch(batch,labels,batchsize)\n",
    "        Batches  = [b,l]\n",
    "    elif typeBatch==\"test\":\n",
    "        batch = testData.sample(batchsize)\n",
    "        labels = testLabels.iloc[batch.index]\n",
    "        b,l=formatBatch(batch,labels,batchsize)\n",
    "        Batches  = [b,l]\n",
    "    else:\n",
    "        print(\"typeBatch needs to be set to one of these values: train, val, test.\")\n",
    "        pass\n",
    "    return Batches\n",
    "\n",
    "def formatBatch(batch,labels,batchsize):\n",
    "    batch = batch.reset_index(drop=True)\n",
    "    labels = labels.reset_index(drop=True)\n",
    "    newl = []\n",
    "    newb = []\n",
    "    for i,b in enumerate(batch['week']):\n",
    "        x = batch['X'][i][1:-1]\n",
    "        y = batch['Y'][i][1:-1]\n",
    "        try:\n",
    "            newb = np.append(newb,[int(b),int(batch['count'][i])/10,(batch['incidence'][i])/100,float(x)/100,float(y)/100])\n",
    "        except:\n",
    "            newb = np.append(newb,[int(b),int(batch['count'][i])/10,(batch['incidence'][i])/100,float(0)/100,float(0)/100])\n",
    "    for ind,l in enumerate(labels['count']):\n",
    "        newl = np.append(newl,[l/10,(labels['incidence'][ind])/100],axis=0)\n",
    "    return np.reshape(newb,(batchsize,5)),np.reshape(newl,(batchsize,2))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[44.          0.1         0.50841713  0.08553381  0.53517689]\n",
      " [46.          0.2         0.60729429  0.12884218  0.4772629 ]\n",
      " [ 1.          0.2         0.57496762  0.06676959  0.49503099]\n",
      " [26.          0.8         0.4423435   0.08985247  0.53127569]\n",
      " [10.          0.1         0.19749402  0.10574879  0.48213055]\n",
      " [37.          0.2         0.72045199  0.10598889  0.50047276]\n",
      " [33.          0.1         0.17952963  0.07988382  0.51093069]\n",
      " [28.          0.5         0.77879305  0.08431752  0.50113092]\n",
      " [45.          0.3         0.3825154   0.11973342  0.51477755]\n",
      " [10.          0.2         0.41174777  0.11872932  0.51153308]\n",
      " [19.          0.6         0.91745255  0.09754013  0.50591582]\n",
      " [44.          1.4         1.01408995  0.06643108  0.51147607]\n",
      " [28.          0.4         0.72354317  0.07215503  0.51536523]\n",
      " [ 4.          0.3         1.00448368  0.11172501  0.49728558]\n",
      " [39.          4.9         0.92885838  0.10237936  0.53497897]\n",
      " [19.          0.3         0.44468154  0.08418972  0.49010402]\n",
      " [43.          0.1         0.37721614  0.10879749  0.49030651]\n",
      " [ 6.          0.3         0.62463342  0.09316856  0.53255589]\n",
      " [21.          0.5         1.03654851  0.07908223  0.52915253]\n",
      " [14.          0.6         0.59010014  0.08068736  0.52334706]\n",
      " [31.          0.1         0.20514457  0.11771923  0.52068518]\n",
      " [13.          0.3         0.28993184  0.07232085  0.51473814]\n",
      " [20.          0.5         0.60792621  0.07194022  0.51027771]\n",
      " [38.          0.7         0.44307353  0.09174547  0.48784481]\n",
      " [48.          0.7         1.11347538  0.12174144  0.51778919]\n",
      " [14.          0.1         0.28657401  0.11007341  0.51319657]\n",
      " [30.          0.2         0.77911959  0.10518859  0.48602711]\n",
      " [44.          0.4         0.81156446  0.13798281  0.52068919]\n",
      " [11.          0.2         0.26798203  0.09754013  0.50591582]\n",
      " [ 2.          0.1         0.2458244   0.08856586  0.53590018]\n",
      " [18.          0.3         0.36730956  0.09808402  0.54298504]\n",
      " [45.          0.3         0.46534147  0.12217863  0.50458881]\n",
      " [37.          0.1         0.38897052  0.10879749  0.49030651]\n",
      " [12.          0.1         0.12507383  0.07194022  0.51027771]\n",
      " [ 3.          0.6         1.22572658  0.08100802  0.5015811 ]\n",
      " [46.          0.4         0.46147316  0.12217863  0.50458881]\n",
      " [17.          0.1         0.30116954  0.10937975  0.48023927]\n",
      " [25.          0.5         1.24077328  0.10693941  0.50908789]\n",
      " [47.          0.3         0.68646531  0.11289489  0.47989032]\n",
      " [35.          0.7         2.50361817  0.06410603  0.50076556]\n",
      " [51.          0.1         0.18811726  0.09363923  0.51022542]\n",
      " [ 3.          0.7         1.11572696  0.08100802  0.5015811 ]\n",
      " [14.          0.1         0.3428898   0.12530822  0.48885736]\n",
      " [34.          0.3         0.7690914   0.08488691  0.48470573]\n",
      " [ 9.          2.5         0.45180244  0.10237936  0.53497897]\n",
      " [52.          0.2         0.63667515  0.12703571  0.48216069]\n",
      " [15.          0.2         0.76277319  0.06947075  0.49928801]\n",
      " [13.          0.3         0.67425513  0.070164    0.52496459]\n",
      " [32.          0.5         0.55583982  0.08518768  0.49490298]\n",
      " [20.          0.2         0.32810568  0.11196307  0.48188053]]\n",
      "batch\n",
      "[44.          0.1         0.50841713  0.08553381  0.53517689]\n",
      "labels\n",
      "[46.          0.2         0.60729429  0.12884218  0.4772629 ]\n",
      "batch\n",
      "[0. 0.]\n",
      "labels\n",
      "[0.1        0.30159847]\n"
     ]
    }
   ],
   "source": [
    "b = getBatches(50,'val')\n",
    "print(b[0])\n",
    "for e in b:\n",
    "    print(\"batch\")\n",
    "    print(e[0])\n",
    "    print(\"labels\")\n",
    "    print(e[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name sample output is illegal; using sample_output instead.\n",
      "INFO:tensorflow:Summary name sample target output is illegal; using sample_target_output instead.\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "%matplotlib notebook\n",
    "learning_rate = 0.05\n",
    "batch_size = 100\n",
    "training_epochs = 100\n",
    "display_step = 1\n",
    "complexInput=False\n",
    "n_hidden_1 = 256 # 1st layer number of neurons\n",
    "n_hidden_2 = 256 # 2nd layer number of neurons\n",
    "n_input = 5 \n",
    "n_classes = 2 \n",
    "step = 1\n",
    "\n",
    "# tf Graph input\n",
    "X = tf.placeholder(\"float32\", [None,n_input])\n",
    "Y = tf.placeholder(\"float32\",[None,n_classes])\n",
    "\n",
    "keep_probability = tf.placeholder(\"float\")\n",
    "\n",
    "weights = {\n",
    "    'h1': tf.Variable(tf.random_normal([n_input, n_hidden_1],stddev=1/n_input)),#**(-1/2))),\n",
    "    'h2': tf.Variable(tf.random_normal([n_hidden_1, n_hidden_2],stddev=1/n_hidden_1)),#**(-1/2))),\n",
    "    'out': tf.Variable(tf.random_normal([n_hidden_2, n_classes],stddev=1/n_hidden_2))#**(-1/2)))\n",
    "}\n",
    "\n",
    "biases = {\n",
    "    'b1': tf.Variable(tf.random_normal([n_hidden_1])),\n",
    "    'b2': tf.Variable(tf.random_normal([n_hidden_2])),\n",
    "    'bout': tf.Variable(tf.random_normal([n_classes]))\n",
    "}\n",
    "\n",
    "def multilayer_perceptron(x):\n",
    "    # Hidden fully connected layer with 256 neurons\n",
    "    layer_1 = tf.nn.dropout(tf.nn.tanh(tf.add(tf.matmul(x, weights['h1']), biases['b1'])),keep_probability)\n",
    "    # Hidden fully connected layer with 256 neurons\n",
    "    layer_2 = tf.nn.dropout(tf.nn.tanh(tf.add(tf.matmul(layer_1, weights['h2']), biases['b2'])),keep_probability)\n",
    "    # Output fully connected layer with a neuron for each class\n",
    "    out_layer = tf.matmul(layer_2, weights['out']) + biases['bout']\n",
    "    return out_layer\n",
    "\n",
    "outP = multilayer_perceptron(X)\n",
    "\n",
    "loss_op = tf.reduce_mean(tf.losses.mean_squared_error(Y,outP))\n",
    "optimizer = tf.train.AdagradOptimizer(learning_rate=learning_rate)\n",
    "train_op = optimizer.minimize(loss_op)\n",
    "\n",
    "tf.summary.scalar(\"loss\", loss_op)\n",
    "tf.summary.scalar(\"sample output\",(outP[0][0])*10)\n",
    "tf.summary.scalar(\"sample target output\",(Y[0][1])*100)\n",
    "\n",
    "merged_summaries = tf.summary.merge_all()\n",
    "\n",
    "train_writer = tf.summary.FileWriter(\"./summaries/trainFFNAreaRisk\", tf.get_default_graph())\n",
    "validation_writer = tf.summary.FileWriter(\"./summaries/validationFFNAreaRisk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    init = tf.initialize_all_variables()\n",
    "    sess.run(init)\n",
    "    saver = tf.train.Saver()\n",
    "    kp=1.0\n",
    "    for n in range(training_epochs):\n",
    "        losses = []\n",
    "        for b in getBatches(batch_size,\"train\"):\n",
    "            _,loss,output,_summaries = sess.run([train_op,loss_op,outP,merged_summaries], feed_dict={X: b[0],Y: b[1],keep_probability:kp})  \n",
    "            lastout = output[0]\n",
    "            losses = np.append(losses,loss)\n",
    "            step = step + 1\n",
    "            train_writer.add_summary(_summaries, step)\n",
    "            if (step%1000==0):\n",
    "                saver.save(sess,\"./log/FFNCountyRisk.ckpt\")\n",
    "        b = getBatches(batch_size,'val')\n",
    "        _,Valloss,output,_Valsummaries = sess.run([train_op,loss_op,outP,merged_summaries], feed_dict={X: np.reshape(b[0],(batch_size,5)),Y: np.reshape(b[1],(batch_size,2)),keep_probability:kp})  \n",
    "        print(\"validation Loss: \",Valloss)\n",
    "        validation_writer.add_summary(_Valsummaries, step)\n",
    "\n",
    "        print(\"epoch: \",n,' - Step: ',step)\n",
    "        print(\"pred: \",lastout)\n",
    "        print(\"true: \",b[1][0])\n",
    "        print(\"loss: \",np.mean(losses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
