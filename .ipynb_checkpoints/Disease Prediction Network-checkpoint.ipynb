{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Vivi\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "%pylab inline\n",
    "import tensorflow as tf\n",
    "#from src.setup.config import db\n",
    "from itertools import groupby\n",
    "import pandas as pd\n",
    "from collections import OrderedDict\n",
    "from datetime import date\n",
    "import pyodbc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing (can be skipped after done once)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "records=pd.read_csv('C:/Users/Vivi/Desktop/hack4health_RKI-data-scripts/survstat/csv/cam_5y.csv',encoding='latin-1')\n",
    "CoNew = pd.read_csv('C:/Users/Vivi/Desktop/hack4health_RKI-data-scripts/DieInfluenza/CoordBins.csv',encoding='latin-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "recordsX=records[['county','week','count','incidence']]\n",
    "gb = recordsX.groupby(['week','county']).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gb.index[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Vivi\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:9: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if __name__ == '__main__':\n"
     ]
    }
   ],
   "source": [
    "newDF = pd.DataFrame(columns=['week','year','county','count','incidence','X','Y','BinX','BinY','LocCounty'])\n",
    "x = 0\n",
    "for i in gb.index:\n",
    "    week = i[0]\n",
    "    counties = i[1]\n",
    "    counts = gb.values[x][0]\n",
    "    incidences = gb.values[x][1]\n",
    "    loc = CoNew[CoNew['county']==counties]\n",
    "    if loc['X'].values>0:\n",
    "        newDF=newDF.append({'week': str(week)[7:],'year':str(week)[:4],'county':str(counties),'count':str(counts),'incidence':str(incidences),'X':loc['X'].values,'Y':loc['Y'].values,'BinX':loc['BinX'].values,'BinY':loc['BinY'].values,'LocCounty':loc['county'].values}, ignore_index=True)    \n",
    "    try:\n",
    "        with open('StomachVirusInput.csv', 'a') as f:\n",
    "            newDF.to_csv(f, header=False)\n",
    "    except:\n",
    "        pass\n",
    "    newDF=pd.DataFrame(columns=['week','year','county','count','incidence','X','Y','BinX','BinY','LocCounty'])\n",
    "    x = x+1\n",
    "#newDF.to_csv('StomachVirusInput.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "newDF = pd.read_csv('StomachVirusInput.csv',encoding='latin-1')\n",
    "newDF.columns = ['ID','week','year','county','count','incidence','X','Y','BinX','BinY','LocCounty']#.set_index('Unnamed: 0')\n",
    "newDF = newDF.dropna()\n",
    "newDF = newDF[['week','year','county','count','incidence','X','Y','BinX','BinY']].reset_index(drop=True)\n",
    "newDF.to_csv('StomachVirusNew.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "newDF = pd.read_csv('StomachVirusInput.csv',encoding='latin-1')\n",
    "newDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "newDF[newDF['year']==2001][newDF[newDF['year']==2001]['week']==2]['incidence']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def FindLabels(data):\n",
    "    #print(data)\n",
    "    LabelDF = pd.DataFrame(columns=['count','incidence','county'])\n",
    "    for w in data.index:\n",
    "        #print(w)\n",
    "        #print(data[data['year']=='2001'][data[data['year']=='2001']['week']=='02'])\n",
    "        try:\n",
    "            thisyear = data.iloc[w]['year']\n",
    "            thisweek = data.iloc[w]['week']\n",
    "            thisarea = data.iloc[w]['county']\n",
    "            if int(thisweek)<52:\n",
    "                nextweek = int(thisweek)+1\n",
    "                nextyear = int(thisyear)\n",
    "            else:\n",
    "                nextweek = 1\n",
    "                nextyear = int(thisyear)+1\n",
    "            nextdf = data[data['year']==nextyear][data[data['year']==nextyear]['week']==nextweek]\n",
    "            nextcount = np.sum(nextdf[nextdf['county']==thisarea]['count'])\n",
    "            nextincidence = np.sum(nextdf[nextdf['county']==thisarea]['incidence'])\n",
    "            nextcounty = nextdf['county']\n",
    "            #print(nextcount)\n",
    "            #print(nextincidence)\n",
    "        except:\n",
    "            nextcount=0\n",
    "            nextindice=0  \n",
    "            nextcounty = data.iloc[w]['county']\n",
    "        LabelDF=LabelDF.append({'count':nextcount,'incidence':nextincidence,'county':nextcounty}, ignore_index=True)\n",
    "    return LabelDF\n",
    "        \n",
    "label = FindLabels(newDF)\n",
    "label.to_csv('StomachOutput.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Processed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Labels=pd.read_csv('StomachOutput.csv',encoding='latin-1').set_index('Unnamed: 0')\n",
    "Data=pd.read_csv('StomachVirusInput.csv',encoding='latin-1').set_index('Unnamed: 0')\n",
    "Data = Data[['week','count','incidence','X','Y']]\n",
    "dfboth = Data.join(Labels,lsuffix='x',how='outer').dropna()\n",
    "Data = dfboth[['week','countx','incidencex','X','Y']].rename(index=str, columns={\"countx\": \"count\", \"incidencex\": \"incidence\"})\n",
    "Labels = dfboth[['count','incidence']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalize Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn.preprocessing import normalize\n",
    "normalized = newDF\n",
    "\"\"\"x = normalized['BinX']\n",
    "normBinX =sklearn.preprocessing.normalize([x],axis=1)\n",
    "normalized['BinX']=normBinX[0]\n",
    "x = normalized['BinY']\n",
    "normBinY =sklearn.preprocessing.normalize([x])\n",
    "normalized['BinY']=normBinY[0]\n",
    "x = normalized['X']\n",
    "normX =sklearn.preprocessing.normalize([x])\n",
    "normalized['X']=normX[0]\n",
    "x = normalized['Y']\n",
    "normY =sklearn.preprocessing.normalize([x])\n",
    "normalized['Y']=normY[0]\"\"\"\n",
    "x = normalized['count']\n",
    "normcount =sklearn.preprocessing.normalize([x])\n",
    "normalized['count']=normcount[0]\n",
    "x = normalized['incidence']\n",
    "normInc =sklearn.preprocessing.normalize([x])\n",
    "normalized['incidence']=normInc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "normLabels = Labels\n",
    "x = normLabels['count']\n",
    "normcount =sklearn.preprocessing.normalize([x])\n",
    "normLabels['count']=normcount[0]\n",
    "x = normLabels['incidence']\n",
    "normInc =sklearn.preprocessing.normalize([x])\n",
    "normLabels['incidence']=normcount[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lenall = Data.shape[0]\n",
    "trainSize = lenall//3*2#define sizes for data sets (train: 2/3, val:1/6, test:1/6 of whole dataset) \n",
    "valSize = lenall//6\n",
    "\n",
    "#Split data into train, val and test set------------------------------------------\n",
    "trainingData = Data.iloc[0:trainSize].reset_index(drop=True)\n",
    "trainingLabels = Labels.iloc[0:trainSize].reset_index(drop=True)\n",
    "validationData = Data.iloc[trainSize+1:trainSize+valSize].reset_index(drop=True)\n",
    "validationLabels = Labels.iloc[trainSize+1:trainSize+valSize].reset_index(drop=True)\n",
    "testData = Data.iloc[trainSize+valSize+1:].reset_index(drop=True)\n",
    "testLabels = Labels.iloc[trainSize+valSize+1:].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Method to get batches------------------------------------------------------------\n",
    "def getBatches(batchsize,typeBatch):\n",
    "    if typeBatch==\"train\":\n",
    "        numBatches = shape(trainingData)[0]//batchsize\n",
    "        Batches=[]\n",
    "        for count in range(numBatches-1):\n",
    "            batch = trainingData[count*batchsize:(count+1)*batchsize]\n",
    "            labels = trainingLabels[count*batchsize:(count+1)*batchsize]\n",
    "            b,l=formatBatch(batch,labels,batchsize)\n",
    "            Batches.append([b,l])\n",
    "    elif typeBatch==\"val\":\n",
    "        batch = validationData.sample(batchsize)\n",
    "        labels = validationLabels.iloc[batch.index]\n",
    "        b,l=formatBatch(batch,labels,batchsize)\n",
    "        Batches  = [b,l]\n",
    "    elif typeBatch==\"test\":\n",
    "        batch = testData.sample(batchsize)\n",
    "        labels = testLabels.iloc[batch.index]\n",
    "        b,l=formatBatch(batch,labels,batchsize)\n",
    "        Batches  = [b,l]\n",
    "    else:\n",
    "        print(\"typeBatch needs to be set to one of these values: train, val, test.\")\n",
    "        pass\n",
    "    return Batches\n",
    "\n",
    "def formatBatch(batch,labels,batchsize):\n",
    "    batch = batch.reset_index(drop=True)\n",
    "    labels = labels.reset_index(drop=True)\n",
    "    newl = []\n",
    "    newb = []\n",
    "    for i,b in enumerate(batch['week']):\n",
    "        x = batch['X'][i][1:-1]\n",
    "        y = batch['Y'][i][1:-1]\n",
    "        try:\n",
    "            newb = np.append(newb,[int(b),int(batch['count'][i]),int(batch['incidence'][i]),float(x),float(y)])\n",
    "        except:\n",
    "            newb = np.append(newb,[int(b),int(batch['count'][i]),int(batch['incidence'][i]),float(0),float(0)])\n",
    "    for ind,l in enumerate(labels['count']):\n",
    "        newl = np.append(newl,[l,labels['incidence'][ind]],axis=0)\n",
    "    return np.reshape(newb,(batchsize,5)),np.reshape(newl,(batchsize,2))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "b = getBatches(4,'train')\n",
    "for e in b:\n",
    "    print(\"batch\")\n",
    "    print(e[0])\n",
    "    print(\"labels\")\n",
    "    print(e[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "%matplotlib notebook\n",
    "learning_rate = 0.05\n",
    "batch_size = 100\n",
    "training_epochs = 100\n",
    "display_step = 1\n",
    "complexInput=False\n",
    "n_hidden_1 = 256 # 1st layer number of neurons\n",
    "n_hidden_2 = 256 # 2nd layer number of neurons\n",
    "n_input = 5 \n",
    "n_classes = 2 \n",
    "step = 1\n",
    "\n",
    "# tf Graph input\n",
    "X = tf.placeholder(\"float32\", [None,n_input])\n",
    "Y = tf.placeholder(\"float32\",[None,n_classes])\n",
    "\n",
    "keep_probability = tf.placeholder(\"float\")\n",
    "\n",
    "weights = {\n",
    "    'h1': tf.Variable(tf.random_normal([n_input, n_hidden_1],stddev=1/n_input)),#**(-1/2))),\n",
    "    'h2': tf.Variable(tf.random_normal([n_hidden_1, n_hidden_2],stddev=1/n_hidden_1)),#**(-1/2))),\n",
    "    'out': tf.Variable(tf.random_normal([n_hidden_2, n_classes],stddev=1/n_hidden_2))#**(-1/2)))\n",
    "}\n",
    "\n",
    "biases = {\n",
    "    'b1': tf.Variable(tf.random_normal([n_hidden_1])),\n",
    "    'b2': tf.Variable(tf.random_normal([n_hidden_2])),\n",
    "    'bout': tf.Variable(tf.random_normal([n_classes]))\n",
    "}\n",
    "\n",
    "def multilayer_perceptron(x):\n",
    "    # Hidden fully connected layer with 256 neurons\n",
    "    layer_1 = tf.nn.dropout(tf.nn.tanh(tf.add(tf.matmul(x, weights['h1']), biases['b1'])),keep_probability)\n",
    "    # Hidden fully connected layer with 256 neurons\n",
    "    layer_2 = tf.nn.dropout(tf.nn.tanh(tf.add(tf.matmul(layer_1, weights['h2']), biases['b2'])),keep_probability)\n",
    "    # Output fully connected layer with a neuron for each class\n",
    "    out_layer = tf.matmul(layer_2, weights['out']) + biases['bout']\n",
    "    return out_layer\n",
    "\n",
    "outP = multilayer_perceptron(X)\n",
    "\n",
    "loss_op = tf.reduce_mean(tf.losses.absolute_difference(outP,Y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "train_op = optimizer.minimize(loss_op)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    init = tf.initialize_all_variables()\n",
    "    sess.run(init)\n",
    "    saver = tf.train.Saver()\n",
    "    kp=1.0\n",
    "    for n in range(training_epochs):\n",
    "        losses = []\n",
    "        for b in getBatches(batch_size,\"train\"):\n",
    "            _,loss,output = sess.run([train_op,loss_op,outP], feed_dict={X: b[0],Y: b[1],keep_probability:kp})  \n",
    "            lastout = output[0]\n",
    "            losses = np.append(losses,loss)\n",
    "        print(\"epoch: \",n)\n",
    "        print(\"pred: \",lastout)\n",
    "        print(\"true: \",b[1][0])\n",
    "        print(\"loss: \",np.mean(losses))\n",
    "\n",
    "                 \n",
    "        step = step + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
