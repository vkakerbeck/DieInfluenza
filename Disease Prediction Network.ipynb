{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Vivi\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "%pylab inline\n",
    "import tensorflow as tf\n",
    "from itertools import groupby\n",
    "import pandas as pd\n",
    "from collections import OrderedDict\n",
    "from datetime import date\n",
    "import pyodbc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing (can be skipped after done once)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "records=pd.read_csv('C:/Users/Vivi/Desktop/hack4health_RKI-data-scripts/survstat/csv/cam_5y.csv',encoding='latin-1')\n",
    "CoNew = pd.read_csv('C:/Users/Vivi/Desktop/hack4health_RKI-data-scripts/DieInfluenza/CoordBins.csv',encoding='latin-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "recordsX=records[['county','week','count','incidence']]\n",
    "gb = recordsX.groupby(['week','county']).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gb.index[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newDF = pd.DataFrame(columns=['week','year','county','count','incidence','X','Y','BinX','BinY','LocCounty'])\n",
    "x = 0\n",
    "for i in gb.index:\n",
    "    week = i[0]\n",
    "    counties = i[1]\n",
    "    counts = gb.values[x][0]\n",
    "    incidences = gb.values[x][1]\n",
    "    loc = CoNew[CoNew['county']==counties]\n",
    "    if loc['X'].values>0:\n",
    "        newDF=newDF.append({'week': str(week)[7:],'year':str(week)[:4],'county':str(counties),'count':str(counts),'incidence':str(incidences),'X':loc['X'].values,'Y':loc['Y'].values,'BinX':loc['BinX'].values,'BinY':loc['BinY'].values,'LocCounty':loc['county'].values}, ignore_index=True)    \n",
    "    try:\n",
    "        with open('StomachVirusInput.csv', 'a') as f:\n",
    "            newDF.to_csv(f, header=False)\n",
    "    except:\n",
    "        pass\n",
    "    newDF=pd.DataFrame(columns=['week','year','county','count','incidence','X','Y','BinX','BinY','LocCounty'])\n",
    "    x = x+1\n",
    "#newDF.to_csv('StomachVirusInput.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "newDF = pd.read_csv('StomachVirusInput.csv',encoding='latin-1')\n",
    "newDF.columns = ['ID','week','year','county','count','incidence','X','Y','BinX','BinY','LocCounty']#.set_index('Unnamed: 0')\n",
    "newDF = newDF.dropna()\n",
    "newDF = newDF[['week','year','county','count','incidence','X','Y','BinX','BinY']].reset_index(drop=True)\n",
    "newDF.to_csv('StomachVirusNew.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "newDF = pd.read_csv('StomachVirusInput.csv',encoding='latin-1')\n",
    "newDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test = newDF[newDF['year']==2001][newDF[newDF['year']==2001]['week']==2]['incidence']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FindLabels(data):\n",
    "    LabelDF = pd.DataFrame(columns=['count','incidence','county'])\n",
    "    for w in data.index:\n",
    "        try:\n",
    "            thisyear = data.iloc[w]['year']\n",
    "            thisweek = data.iloc[w]['week']\n",
    "            thisarea = data.iloc[w]['county']\n",
    "            if int(thisweek)<52:\n",
    "                nextweek = int(thisweek)+1\n",
    "                nextyear = int(thisyear)\n",
    "            else:\n",
    "                nextweek = 1\n",
    "                nextyear = int(thisyear)+1\n",
    "            nextdf = data[data['year']==nextyear][data[data['year']==nextyear]['week']==nextweek]\n",
    "            nextcount = np.sum(nextdf[nextdf['county']==thisarea]['count'])\n",
    "            if nextcount>0:\n",
    "                nextincidence = np.sum(nextdf[nextdf['county']==thisarea]['incidence'])\n",
    "                nextcounty = nextdf[nextdf['county']==thisarea]['county'].values[0]\n",
    "            else:\n",
    "                nextincidence=0  \n",
    "                nextcounty = 'no Data'\n",
    "        except:\n",
    "            nextcount=0\n",
    "            nextincidence=0  \n",
    "            nextcounty = 'no Data'\n",
    "        LabelDF=LabelDF.append({'count':nextcount,'incidence':nextincidence,'county':nextcounty}, ignore_index=True)\n",
    "        if (w % 1000 == 0):\n",
    "            print(w)\n",
    "    return LabelDF\n",
    "        \n",
    "label = FindLabels(newDF)\n",
    "label.to_csv('StomachOutput.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Processed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Labels=pd.read_csv('StomachOutput.csv',encoding='latin-1').set_index('Unnamed: 0')\n",
    "Data=pd.read_csv('StomachVirusInput.csv',encoding='latin-1').set_index('Unnamed: 0')\n",
    "Data = Data[['week','year','count','incidence','X','Y','county','BinX','BinY']]\n",
    "dfboth = Data.join(Labels,lsuffix='x',how='outer').dropna()\n",
    "Data = dfboth[['week','year','countx','incidencex','X','Y','countyx','BinX','BinY']].rename(index=str, columns={\"countx\": \"count\", \"incidencex\": \"incidence\"})\n",
    "Labels = dfboth[['count','incidence']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Vivi\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:15: RuntimeWarning: invalid value encountered in true_divide\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    }
   ],
   "source": [
    "allMaps = []\n",
    "for m in range(15):\n",
    "    for n in range(52):\n",
    "        year = 2001+m\n",
    "        thisweek = Data[Data['year']==year][Data[Data['year']==year]['week']==n]\n",
    "        thisweek = thisweek.reset_index(drop=True)\n",
    "        currentMap = np.zeros((16,16))\n",
    "        currentCount = np.zeros((16,16))\n",
    "        for i in range(shape(thisweek)[0]):\n",
    "            inc = thisweek['incidence'][i]\n",
    "            x = int(thisweek['BinX'][i][1:-1])\n",
    "            y = int(thisweek['BinY'][i][1:-1])\n",
    "            currentMap[x][y]=currentMap[x][y]+inc\n",
    "            currentCount[x][y]=currentCount[x][y]+1\n",
    "        currentMap = currentMap/currentCount\n",
    "        allMaps.append(np.nan_to_num(currentMap))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalize Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn.preprocessing import normalize\n",
    "normalized = newDF\n",
    "\"\"\"x = normalized['BinX']\n",
    "normBinX =sklearn.preprocessing.normalize([x],axis=1)\n",
    "normalized['BinX']=normBinX[0]\n",
    "x = normalized['BinY']\n",
    "normBinY =sklearn.preprocessing.normalize([x])\n",
    "normalized['BinY']=normBinY[0]\n",
    "x = normalized['X']\n",
    "normX =sklearn.preprocessing.normalize([x])\n",
    "normalized['X']=normX[0]\n",
    "x = normalized['Y']\n",
    "normY =sklearn.preprocessing.normalize([x])\n",
    "normalized['Y']=normY[0]\"\"\"\n",
    "x = normalized['count']\n",
    "normcount =sklearn.preprocessing.normalize([x])\n",
    "normalized['count']=normcount[0]\n",
    "x = normalized['incidence']\n",
    "normInc =sklearn.preprocessing.normalize([x])\n",
    "normalized['incidence']=normInc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "normLabels = Labels\n",
    "x = normLabels['count']\n",
    "normcount =sklearn.preprocessing.normalize([x])\n",
    "normLabels['count']=normcount[0]\n",
    "x = normLabels['incidence']\n",
    "normInc =sklearn.preprocessing.normalize([x])\n",
    "normLabels['incidence']=normcount[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lenall = Data.shape[0]\n",
    "trainSize = lenall//3*2#define sizes for data sets (train: 2/3, val:1/6, test:1/6 of whole dataset) \n",
    "valSize = 0#lenall//6\n",
    "\n",
    "#Split data into train, val and test set------------------------------------------\n",
    "trainingData = Data.iloc[0:trainSize].reset_index(drop=True)\n",
    "trainingLabels = Labels.iloc[0:trainSize].reset_index(drop=True)\n",
    "validationData = Data.iloc[trainSize+1:trainSize+valSize].reset_index(drop=True)\n",
    "validationLabels = Labels.iloc[trainSize+1:trainSize+valSize].reset_index(drop=True)\n",
    "testData = Data.iloc[trainSize+valSize+1:].reset_index(drop=True)\n",
    "testLabels = Labels.iloc[trainSize+valSize+1:].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Method to get batches------------------------------------------------------------\n",
    "def getBatches(batchsize,typeBatch):\n",
    "    if typeBatch==\"train\":\n",
    "        numBatches = shape(trainingData)[0]//batchsize\n",
    "        Batches=[]\n",
    "        for count in range(numBatches-1):\n",
    "            batch = trainingData[count*batchsize:(count+1)*batchsize]\n",
    "            labels = trainingLabels[count*batchsize:(count+1)*batchsize]\n",
    "            b,l=formatBatch(batch,labels,batchsize)\n",
    "            newb = addGlobalCNN(b,batch.reset_index(),batchsize,batch['week'].reset_index(),batch['year'].reset_index())\n",
    "            #print(newb)\n",
    "            Batches.append([newb,l])\n",
    "    elif typeBatch==\"val\":\n",
    "        batch = validationData.sample(batchsize)\n",
    "        labels = validationLabels.iloc[batch.index]\n",
    "        b,l=formatBatch(batch,labels,batchsize)\n",
    "        Batches  = [b,l]\n",
    "    elif typeBatch==\"test\":\n",
    "        batch = testData.sample(batchsize)\n",
    "        labels = testLabels.iloc[batch.index]\n",
    "        b,l=formatBatch(batch,labels,batchsize)\n",
    "        Batches  = [b,l]\n",
    "    else:\n",
    "        print(\"typeBatch needs to be set to one of these values: train, val, test.\")\n",
    "        pass\n",
    "    return Batches\n",
    "def getLastWeek(thisweek,thisyear):\n",
    "    weekdata = Data[Data['year']==thisyear][Data[Data['year']==thisyear]['week']==thisweek]\n",
    "    #print(weekdata)\n",
    "    size = shape(weekdata)[0]\n",
    "    batch = formatBatch(weekdata,weekdata,size)\n",
    "    return batch,size,weekdata['countyx']\n",
    "\n",
    "def formatBatch(batch,labels,batchsize):\n",
    "    batch = batch.reset_index(drop=True)\n",
    "    labels = labels.reset_index(drop=True)\n",
    "    newl = []\n",
    "    newb = []\n",
    "    for i,b in enumerate(batch['week']):\n",
    "        x = batch['X'][i][1:-1]\n",
    "        y = batch['Y'][i][1:-1]\n",
    "        try:\n",
    "            newb = np.append(newb,[int(b),int(batch['count'][i])/10,(batch['incidence'][i])/100,float(x)/100,float(y)/100])\n",
    "        except:\n",
    "            newb = np.append(newb,[int(b),int(batch['count'][i])/10,(batch['incidence'][i])/100,float(0)/100,float(0)/100])\n",
    "    for ind,l in enumerate(labels['count']):\n",
    "        newl = np.append(newl,[l/10,(labels['incidence'][ind])/100],axis=0)\n",
    "    return np.reshape(newb,(batchsize,5)),np.reshape(newl,(batchsize,2))\n",
    "def addGlobalCNN(batch,dfBatch,batchsize,week,year):\n",
    "    records = []\n",
    "    newb = []\n",
    "    for i in range(batchsize):\n",
    "        thisweek = week['week'][i]\n",
    "        thisyear = year['year'][i]\n",
    "        MapNum = (thisyear-2001)*52+thisweek\n",
    "        x = dfBatch['BinX'][i]\n",
    "        y = dfBatch['BinY'][i]\n",
    "        binrisk = allMaps[MapNum][int(x[1:-1])][int(y[1:-1])]\n",
    "        newb = np.append(batch[i],binrisk/100)\n",
    "        records.append(newb)\n",
    "    return records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = getBatches(50,'train')\n",
    "#print(b)\n",
    "for e in b:\n",
    "    print(\"batch\")\n",
    "    print(e[0])\n",
    "    print(\"labels\")\n",
    "    print(e[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "%matplotlib notebook\n",
    "learning_rate = 0.05\n",
    "batch_size = 20\n",
    "training_epochs = 100\n",
    "display_step = 1\n",
    "complexInput=False\n",
    "n_hidden_1 = 250 # 1st layer number of neurons\n",
    "n_hidden_2 = 250 # 2nd layer number of neurons\n",
    "n_input = 6 \n",
    "n_classes = 2 \n",
    "step = 1\n",
    "\n",
    "# tf Graph input\n",
    "X = tf.placeholder(\"float32\", [None,n_input])\n",
    "Y = tf.placeholder(\"float32\",[None,n_classes])\n",
    "\n",
    "keep_probability = tf.placeholder(\"float\")\n",
    "\n",
    "weights = {\n",
    "    'h1': tf.Variable(tf.random_normal([n_input, n_hidden_1],stddev=1/n_input)),#**(-1/2))),\n",
    "    'h2': tf.Variable(tf.random_normal([n_hidden_1, n_hidden_2],stddev=1/n_hidden_1)),#**(-1/2))),\n",
    "    'out': tf.Variable(tf.random_normal([n_hidden_2, n_classes],stddev=1/n_hidden_2))#**(-1/2)))\n",
    "}\n",
    "\n",
    "biases = {\n",
    "    'b1': tf.Variable(tf.random_normal([n_hidden_1])),\n",
    "    'b2': tf.Variable(tf.random_normal([n_hidden_2])),\n",
    "    'bout': tf.Variable(tf.random_normal([n_classes]))\n",
    "}\n",
    "\n",
    "def multilayer_perceptron(x):\n",
    "    # Hidden fully connected layer with 256 neurons\n",
    "    layer_1 = tf.nn.dropout(tf.nn.tanh(tf.add(tf.matmul(x, weights['h1']), biases['b1'])),keep_probability)\n",
    "    # Hidden fully connected layer with 256 neurons\n",
    "    layer_2 = tf.nn.dropout(tf.nn.tanh(tf.add(tf.matmul(layer_1, weights['h2']), biases['b2'])),keep_probability)\n",
    "    # Output fully connected layer with a neuron for each class\n",
    "    out_layer = tf.matmul(layer_2, weights['out']) + biases['bout']\n",
    "    return out_layer\n",
    "\n",
    "outP = multilayer_perceptron(X)\n",
    "\n",
    "loss_op = tf.reduce_mean(tf.losses.mean_squared_error(Y,outP))\n",
    "optimizer = tf.train.AdagradOptimizer(learning_rate=learning_rate)\n",
    "train_op = optimizer.minimize(loss_op)\n",
    "\n",
    "#tf.summary.scalar(\"loss\", loss_op)\n",
    "#tf.summary.scalar(\"sample output\",(outP[0][0])*10)\n",
    "#tf.summary.scalar(\"sample target output\",(Y[0][1])*100)\n",
    "\n",
    "#merged_summaries = tf.summary.merge_all()\n",
    "\n",
    "#train_writer = tf.summary.FileWriter(\"./summaries/trainFFNAreaRisk\", tf.get_default_graph())\n",
    "#validation_writer = tf.summary.FileWriter(\"./summaries/validationFFNAreaRisk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.28091413 0.7750211 ]\n",
      " [0.30363613 0.6703932 ]\n",
      " [0.29868233 0.6039038 ]\n",
      " [0.34543115 0.764542  ]\n",
      " [0.41057765 0.78798497]\n",
      " [0.47739643 0.7706523 ]\n",
      " [0.40196118 0.62850547]\n",
      " [0.34198827 0.5970838 ]\n",
      " [0.47197253 0.66380215]\n",
      " [0.40195996 0.72110975]\n",
      " [0.41442448 0.7231797 ]\n",
      " [0.32539755 0.5979036 ]\n",
      " [0.4166556  0.7860575 ]\n",
      " [0.35641596 0.621657  ]\n",
      " [0.33570763 0.61365235]\n",
      " [0.40221646 0.6691686 ]\n",
      " [0.8882832  0.9330362 ]\n",
      " [0.52288616 0.6596755 ]\n",
      " [0.3280705  0.59353745]\n",
      " [0.38995954 0.6856353 ]]\n",
      "epoch:  0  - Step:  211651\n",
      "pred:  [0.34543115 0.764542  ]\n",
      "true:  [0.2        0.70842666]\n",
      "loss:  0.1522153963272163\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-30-70ff189bc697>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mn\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtraining_epochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m         \u001b[0mlosses\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[0mb\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mgetBatches\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"train\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m             \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtrain_op\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mloss_op\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0moutP\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mY\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mkeep_probability\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mkp\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-6-c20f21f13644>\u001b[0m in \u001b[0;36mgetBatches\u001b[1;34m(batchsize, typeBatch)\u001b[0m\n\u001b[0;32m      7\u001b[0m             \u001b[0mbatch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrainingData\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcount\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mbatchsize\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcount\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mbatchsize\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m             \u001b[0mlabels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrainingLabels\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcount\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mbatchsize\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcount\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mbatchsize\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m             \u001b[0mb\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0ml\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mformatBatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbatchsize\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m             \u001b[0mnewb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0maddGlobalCNN\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbatchsize\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'week'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'year'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m             \u001b[1;31m#print(newb)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-6-c20f21f13644>\u001b[0m in \u001b[0;36mformatBatch\u001b[1;34m(batch, labels, batchsize)\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mformatBatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbatchsize\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 35\u001b[1;33m     \u001b[0mbatch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     36\u001b[0m     \u001b[0mlabels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m     \u001b[0mnewl\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36mreset_index\u001b[1;34m(self, level, drop, inplace, col_level, col_fill)\u001b[0m\n\u001b[0;32m   2886\u001b[0m             \u001b[0mnew_obj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2887\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2888\u001b[1;33m             \u001b[0mnew_obj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2889\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2890\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0m_maybe_casted_values\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36mcopy\u001b[1;34m(self, deep)\u001b[0m\n\u001b[0;32m   3430\u001b[0m         \u001b[0mcopy\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0mtype\u001b[0m \u001b[0mof\u001b[0m \u001b[0mcaller\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3431\u001b[0m         \"\"\"\n\u001b[1;32m-> 3432\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdeep\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdeep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3433\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_constructor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__finalize__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3434\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\internals.py\u001b[0m in \u001b[0;36mcopy\u001b[1;34m(self, deep, mgr)\u001b[0m\n\u001b[0;32m   3434\u001b[0m             \u001b[0mnew_axes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3435\u001b[0m         return self.apply('copy', axes=new_axes, deep=deep,\n\u001b[1;32m-> 3436\u001b[1;33m                           do_integrity_check=False)\n\u001b[0m\u001b[0;32m   3437\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3438\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mas_matrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mitems\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\internals.py\u001b[0m in \u001b[0;36mapply\u001b[1;34m(self, f, axes, filter, do_integrity_check, consolidate, **kwargs)\u001b[0m\n\u001b[0;32m   3095\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmake_empty\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxes\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3096\u001b[0m         bm = self.__class__(result_blocks, axes or self.axes,\n\u001b[1;32m-> 3097\u001b[1;33m                             do_integrity_check=do_integrity_check)\n\u001b[0m\u001b[0;32m   3098\u001b[0m         \u001b[0mbm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_consolidate_inplace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3099\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mbm\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\internals.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, blocks, axes, do_integrity_check, fastpath)\u001b[0m\n\u001b[0;32m   2797\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_consolidate_check\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2798\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2799\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_rebuild_blknos_and_blklocs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2800\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2801\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mmake_empty\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\internals.py\u001b[0m in \u001b[0;36m_rebuild_blknos_and_blklocs\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   2886\u001b[0m             \u001b[0mrl\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mblk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmgr_locs\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2887\u001b[0m             \u001b[0mnew_blknos\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mrl\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mblkno\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2888\u001b[1;33m             \u001b[0mnew_blklocs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mrl\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2889\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2890\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mnew_blknos\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0many\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    init = tf.initialize_all_variables()\n",
    "    sess.run(init)\n",
    "    saver = tf.train.Saver()\n",
    "    kp=1.0\n",
    "    samplePred = []\n",
    "    sample = []\n",
    "    allLoss = []\n",
    "    for n in range(training_epochs):\n",
    "        losses = []\n",
    "        for b in getBatches(batch_size,\"train\"):\n",
    "            _,loss,output = sess.run([train_op,loss_op,outP], feed_dict={X: b[0],Y: b[1],keep_probability:kp})  \n",
    "\n",
    "            losses = np.append(losses,loss)\n",
    "            step = step + 1\n",
    "            #train_writer.add_summary(_summaries, step)\n",
    "            #if (step%1000==0):\n",
    "             #   saver.save(sess,\"./log/FFNCountyRisk.ckpt\")\n",
    "        #b = getBatches(batch_size,'val')\n",
    "        #_,Valloss,output,_Valsummaries = sess.run([train_op,loss_op,outP,merged_summaries], feed_dict={X: np.reshape(b[0],(batch_size,5)),Y: np.reshape(b[1],(batch_size,2)),keep_probability:kp})  \n",
    "        #print(\"validation Loss: \",Valloss)\n",
    "        #validation_writer.add_summary(_Valsummaries, step)\n",
    "        print(output)\n",
    "        lastout = output[3]\n",
    "        samplePred = np.append(samplePred,output)#lastout[1])\n",
    "        sample = np.append(sample,b[1])\n",
    "        allLoss = np.append(allLoss,np.mean(losses))\n",
    "        print(\"epoch: \",n,' - Step: ',step)\n",
    "        print(\"pred: \",lastout)\n",
    "        print(\"true: \",b[1][2])\n",
    "        print(\"loss: \",np.mean(losses))\n",
    "        #print(losses[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40.0\n",
      "0\n",
      "0.27724987268447876\n",
      "0.7669916152954102\n",
      "1\n",
      "0.3039957880973816\n",
      "0.6681524515151978\n",
      "2\n",
      "0.3098495602607727\n",
      "0.6049075722694397\n",
      "3\n",
      "0.34203600883483887\n",
      "0.7560332417488098\n",
      "4\n",
      "0.39929091930389404\n",
      "0.7782647013664246\n",
      "5\n",
      "0.4647103548049927\n",
      "0.7637075185775757\n",
      "6\n",
      "0.402984619140625\n",
      "0.6261228919029236\n",
      "7\n",
      "0.34808260202407837\n",
      "0.597250759601593\n",
      "8\n",
      "0.4682995676994324\n",
      "0.6587813496589661\n",
      "9\n",
      "0.39255183935165405\n",
      "0.7144127488136292\n",
      "10\n",
      "0.4071752429008484\n",
      "0.7187098860740662\n",
      "11\n",
      "0.3327401876449585\n",
      "0.5982867479324341\n",
      "12\n",
      "0.4125635027885437\n",
      "0.774476170539856\n",
      "13\n",
      "0.3607600927352905\n",
      "0.6215968728065491\n",
      "14\n",
      "0.34033429622650146\n",
      "0.6137465238571167\n",
      "15\n",
      "0.3987804055213928\n",
      "0.6640257239341736\n",
      "16\n",
      "0.8928439617156982\n",
      "0.920444905757904\n",
      "17\n",
      "0.5172681212425232\n",
      "0.6526880860328674\n",
      "18\n",
      "0.33602553606033325\n",
      "0.5938740968704224\n",
      "19\n",
      "0.3906683325767517\n",
      "0.6805176138877869\n",
      "20\n",
      "0.2687119245529175\n",
      "0.7584760785102844\n",
      "21\n",
      "0.2911517024040222\n",
      "0.662869930267334\n",
      "22\n",
      "0.3020186424255371\n",
      "0.5922251343727112\n"
     ]
    }
   ],
   "source": [
    "#sp = samplePred\n",
    "#s = sample\n",
    "print(shape(s)[0]/23)\n",
    "for i in range(int(shape(sp)[0]/(batch_size*2))):\n",
    "    print(i)\n",
    "    print(sp[i*2])\n",
    "    #print(s[i*2])\n",
    "    print(sp[i*2+1])\n",
    "    #print(s[i*2+1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23.0"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shape(samplePred)[0]/(batch_size*2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make New Predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thisweek = 52\n",
    "thisyear = 2016\n",
    "with tf.Session() as sess:\n",
    "    init = tf.initialize_all_variables()\n",
    "    sess.run(init)\n",
    "    saver = tf.train.Saver()\n",
    "    kp=1.0\n",
    "    saver.restore(sess, \"C:/Users/Vivi/Desktop/hack4health_RKI-data-scripts/DieInfluenza/log/FFNCountyRisk.ckpt\")\n",
    "    print(\"model loaded\")\n",
    "    #b,s,c = getLastWeek(thisweek,thisyear)\n",
    "    batch_size = s\n",
    "    _,loss,output= sess.run([train_op,loss_op,outP], feed_dict={X: newinput,Y: b[1],keep_probability:kp}) \n",
    "    print(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = pd.DataFrame(np.multiply(output,100))\n",
    "counties = pd.DataFrame(c)\n",
    "\n",
    "d = predicted.join(counties.reset_index(),lsuffix='x')\n",
    "#d = d[['countyx','1']]\n",
    "d.columns = ['count','incidence','ID','county']\n",
    "d['week']=1#thisweek+1\n",
    "d['year']=thisyear+1\n",
    "d = d[['count','incidence','county','week','year']]\n",
    "d.to_csv('Prediction_1_2017.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mses = []\n",
    "for b in getBatches(batch_size,'train'):\n",
    "    A = [b[0][0][1],b[0][0][2]]\n",
    "    B = b[1][0]\n",
    "    mse = ((A-B)**2).mean()\n",
    "    mses.append(mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
