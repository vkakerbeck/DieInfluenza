{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Vivi\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'src'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-32ae2256c44b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmagic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'pylab inline'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0msrc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msetup\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconfig\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdb\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mitertools\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mgroupby\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'src'"
     ]
    }
   ],
   "source": [
    "%pylab inline\n",
    "import tensorflow as tf\n",
    "from src.setup.config import db\n",
    "from itertools import groupby\n",
    "import pandas as pd\n",
    "from collections import OrderedDict\n",
    "from datetime import date\n",
    "import pyodbc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#from tensorflow.contrib.tensorboard.plugins import projector\n",
    "tf.reset_default_graph()\n",
    "%matplotlib notebook\n",
    "learning_rate = 0.05\n",
    "training_epochs = 10000\n",
    "batch_size = 50\n",
    "display_step = 1\n",
    "# Network Parameters\n",
    "n_hidden_1 = 256 # 1st layer number of neurons\n",
    "n_hidden_2 = 256 # 2nd layer number of neurons\n",
    "n_input = 9999 # data input \n",
    "n_classes = dictsize # total classes (0-9999 drgs)\n",
    "step = 0\n",
    "\n",
    "# tf Graph input\n",
    "\n",
    "#X = tf.placeholder(\"float\", [None,n_input])\n",
    "X = tf.placeholder(\"int32\", [None,numICDs])\n",
    "Y = tf.placeholder(\"int32\",[None,1])\n",
    "\n",
    "\n",
    "keep_probability = tf.placeholder(\"float\")\n",
    "# Store layers weight & bias\n",
    "weights = {\n",
    "    'h1': tf.Variable(tf.random_normal([n_input, n_hidden_1])),\n",
    "    'h2': tf.Variable(tf.random_normal([n_hidden_1, n_hidden_2])),\n",
    "    'out': tf.Variable(tf.random_normal([n_hidden_2, n_classes]))\n",
    "}\n",
    "\n",
    "biases = {\n",
    "    'b1': tf.Variable(tf.random_normal([n_hidden_1])),\n",
    "    'b2': tf.Variable(tf.random_normal([n_hidden_2])),\n",
    "    'out': tf.Variable(tf.random_normal([n_classes]))\n",
    "}\n",
    "# Create model\n",
    "\n",
    "def multilayer_perceptron(x):\n",
    "    # Hidden fully connected layer with 256 neurons\n",
    "    layer_1 = tf.nn.dropout(tf.nn.tanh(tf.add(tf.matmul(inputVec, weights['h1']), biases['b1'])),keep_probability)\n",
    "    # Hidden fully connected layer with 256 neurons\n",
    "    layer_2 = tf.nn.dropout(tf.nn.tanh(tf.add(tf.matmul(layer_1, weights['h2']), biases['b2'])),keep_probability)\n",
    "    # Output fully connected layer with a neuron for each class\n",
    "    out_layer = tf.matmul(layer_2, weights['out']) + biases['out']\n",
    "    return out_layer\n",
    "\n",
    "# Construct model\n",
    "out = multilayer_perceptron(X)\n",
    "#Define accuracy (average number of correctly predicted treatments (only trues count))\n",
    "pred = tf.argmax(tf.nn.softmax(out),axis=1,output_type=tf.int32)\n",
    "Comparison=tf.equal(tf.reshape(pred,(batch_size,1)),Y)\n",
    "RightPred=tf.reduce_sum(tf.reduce_sum(tf.cast(Comparison, tf.float32),1))/batch_size\n",
    "\n",
    "diff = tf.reduce_mean(tf.losses.absolute_difference(tf.reshape(pred,(batch_size,1)),Y))\n",
    "# Define loss and optimizer\n",
    "loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
    "    logits=out, labels=targetVec))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "train_op = optimizer.minimize(loss_op)\n",
    "\n",
    "\n",
    "tf.summary.scalar(\"cross_entropy\", loss_op)\n",
    "tf.summary.scalar(\"accuracy\",RightPred)\n",
    "tf.summary.scalar(\"absDiff\",diff)\n",
    "\n",
    "merged_summaries = tf.summary.merge_all()\n",
    "# Initializing the variables\n",
    "#init = tf.global_variables_initializer()\n",
    "\n",
    "train_writer = tf.summary.FileWriter(\"./DRGsummaries/trainICDtoDict\", tf.get_default_graph())\n",
    "validation_writer = tf.summary.FileWriter(\"./DRGsummaries/valICDtoDict\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    saver = tf.train.Saver()\n",
    "    kp=0.8\n",
    "    for n in range(training_epochs):   \n",
    "        icd, ops = getBatch(batch_size,\"train\",'dictionary')\n",
    "        _,loss,_summaries = sess.run([train_op,loss_op,merged_summaries], feed_dict={X: icd,Y: np.reshape(ops,(batch_size,1)),keep_probability:kp})             \n",
    "\n",
    "        train_writer.add_summary(_summaries, step)\n",
    "        if (step % 50 == 0): \n",
    "            kp=1.0\n",
    "            icd, ops = getBatch(batch_size,\"val\",'dictionary')\n",
    "            _,acc,loss_val,_summaries,absdiff = sess.run([train_op,RightPred,loss_op,merged_summaries,diff], feed_dict={X: icd,\n",
    "                                                            Y: np.reshape(ops,(batch_size,1)),keep_probability:kp})\n",
    "            print(\"Accuracy: \",acc)\n",
    "            print(\"Cross Entropy: \",loss_val)\n",
    "            print(\"Diff: \",absdiff)\n",
    "\n",
    "            validation_writer.add_summary(_summaries, step)\n",
    "        #if (step%1000==0):\n",
    "         #   saver.save(sess,\"./log/predDRGfromICDDict.ckpt\")\n",
    "                 \n",
    "        step = step + 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
